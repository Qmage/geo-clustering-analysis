{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Geolocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook,output_file, show, save, curdoc, push_notebook\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool, HBox, VBoxForm, Circle, DataRange1d, \\\n",
    "PanTool, WheelZoomTool, BoxSelectTool,  GMapPlot, GMapOptions\n",
    "from bokeh.models.widgets import Slider, Select, TextInput, CheckboxButtonGroup\n",
    "from ipywidgets import interact,widgets\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import ConvexHull\n",
    "from collections import Counter\n",
    "import time\n",
    "from random import randint\n",
    "import string\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from matplotlib.path import Path\n",
    "from pytz import timezone\n",
    "from math import floor\n",
    "from datetime import time as timeclass\n",
    "from urllib import parse\n",
    "import requests\n",
    "from geopy.distance import vincenty\n",
    "import math\n",
    "import foliumy\n",
    "from foliumy.element import IFrame\n",
    "from IPython.core.display import display, HTML, clear_output\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "import os.path\n",
    "\n",
    "css = \"\"\"\n",
    "<style>\n",
    ".dropdown-menu{\n",
    "    height: auto;\n",
    "    max-height: 150px;\n",
    "    overflow-x: hidden;\n",
    "}\n",
    "</style>\n",
    "\"\"\"\n",
    "HTML(css)\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def populate_venuetype(x):\n",
    "    types = ['dining','entertainment','social','sports','corporate','retail']\n",
    "    result = []\n",
    "    x = [list(y) for y in set(tuple(y) for y in x)]\n",
    "    for y in x:\n",
    "        result.append(y + [types[randint(0,5)]])\n",
    "    return result\n",
    "\n",
    "def map_dbscan(latlong_list, ep, mins):\n",
    "    labels = DBSCAN(eps=ep, min_samples=mins).fit([ [i[0],i[1]] for i in latlong_list] ).labels_\n",
    "    label_dict = {}\n",
    "    for i,j in zip(labels, latlong_list):\n",
    "        if i not in label_dict:\n",
    "            label_dict[i] = {}\n",
    "            label_dict[i]['points'] = [ { 'x':float(j[1]), 'y':float(j[0]),'z':1,'type':j[2] } ]\n",
    "        else:\n",
    "            label_dict[i]['points'].append( { 'x':float(j[1]), 'y':float(j[0]),'z':1,'type':j[2] } )\n",
    "    for i in label_dict:\n",
    "        count = Counter([j['type'] for j in label_dict[i]['points']])\n",
    "        label_dict[i]['common_type'] = count.most_common()[0][0]\n",
    "        \n",
    "        points = np.array( [[j['x'],j['y']] for j in label_dict[i]['points']] )\n",
    "        if len(points)>2:\n",
    "            hull = ConvexHull(points)\n",
    "            label_dict[i]['boundaries'] = {}\n",
    "            label_dict[i]['boundaries']['simplices'] = points[hull.simplices]\n",
    "            label_dict[i]['boundaries']['vertices'] = points[hull.vertices]\n",
    "        \n",
    "    return label_dict\n",
    "\n",
    "def get_patches(groupings):\n",
    "    colors = [\"#000000\",\"#008080\",\"#0000FF\",\"#FFFF00\",\"#00FF00\",\"#800080\", \"#40e0d0\",\"#000FF0\",\"#066FF0\",\"#366FF0\"]\n",
    "    type_to_color = {'dining':0,'entertainment':1,'social':2,'sports':3,'corporate':4,'retail':5}\n",
    "\n",
    "    group = {}\n",
    "    for g in groupings:\n",
    "        group[g] = {}\n",
    "        group[g]['x'] = []\n",
    "        group[g]['y'] = []\n",
    "        group[g]['color'] = []\n",
    "        for c in groupings[g]:\n",
    "            if 'boundaries' in groupings[g][c] and c != -1:\n",
    "                group[g]['x'].append([ j[0] for j in groupings[g][c]['boundaries']['vertices'] ])\n",
    "                group[g]['y'].append([ j[1] for j in groupings[g][c]['boundaries']['vertices'] ])\n",
    "                group[g]['color'].append([ colors[type_to_color[groupings[g][c]['common_type']] ] ])\n",
    "    return group\n",
    "\n",
    "def get_points(groupings):\n",
    "    group = {}\n",
    "    for g in groupings:\n",
    "        group[g] = {}\n",
    "        group[g]['x'] = []\n",
    "        group[g]['y'] = []\n",
    "        for c in groupings[g]:\n",
    "            group[g]['x'] = group[g]['x'] + [ j['x'] for j in groupings[g][c]['points'] ]\n",
    "            group[g]['y'] = group[g]['y'] + [ j['y'] for j in groupings[g][c]['points'] ]\n",
    "    return group\n",
    "\n",
    "def checkCoordinate(y, travel_mode, userid):\n",
    "        result = \"User \" + travel_mode + \", not in cluster\"\n",
    "        for i in top_users[userid]:\n",
    "            if i != -1:\n",
    "                if Path( top_users[userid][i]['boundaries']['vertices'] ).contains_point(y):\n",
    "                    result = \"User \" + travel_mode + \", located in cluster [\" + str(i) + \"], recommending \" + \\\n",
    "                    top_users[userid][i][\"common_type\"] + ' promotions'\n",
    "        return result\n",
    "checkin = sc.textFile(\"geodata/checkins.csv\").map(lambda x : x.split(\",\")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell below performs the clustering algorithm (this takes a few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topuser_ids = [842, 22, 4985, 4902, 4929, 1201, 578, 111, 1274, 1940, 3480, 5001, 4952]\n",
    "if os.path.isfile(\"top_users.pickle\"):\n",
    "    top_users = pickle.load( open( \"top_users.pickle\", \"rb\" ) )\n",
    "else:\n",
    "    \n",
    "    checkin_users = checkin.map(lambda x: (x[5], [x[6],x[7]] ) ).groupByKey().\\\n",
    "    mapValues(lambda x : populate_venuetype(x)).cache()\n",
    "    checkin_users_cluster = checkin_users.mapValues(lambda x : map_dbscan(x, 0.004, 4) ).cache()\n",
    "    all_users_clusters = checkin_users_cluster.collect()\n",
    "\n",
    "    #large_clusters = checkin_users_cluster.map(lambda x: (sum( len(x[1][i]['points']) for i in x[1] if i != -1 ), x[0] ) ).\\\n",
    "    #sortByKey(ascending=False).take(20)\n",
    "    #topuser_ids = [int(c[1]) for c in large_clusters]\n",
    "    top_users = checkin_users_cluster.map(lambda x: (int(x[0]), x[1]) ).filter(lambda x: x[0] in topuser_ids).collectAsMap()\n",
    "    pickle.dump( top_users, open(\"top_users.pickle\", \"wb\") )\n",
    "users_patches = get_patches(top_users)\n",
    "users_points = get_points(top_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_geolocation_cluster():\n",
    "    p = figure(title=\"Click on button \\\"Simulate\\\" to begin\", title_text_font_size='14pt', x_range=(-74.081,-73.861), y_range=(40.7,40.865), plot_width=740, plot_height=741)\n",
    "    p.image_url(url=[\"https://maps.googleapis.com/maps/api/staticmap?center=Manhattan,New+York,NY&zoom=12&size=640x640&maptype=roadmap\"],\\\n",
    "                x=-74.081, y=40.865)\n",
    "\n",
    "    source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=[0],\n",
    "                y=[0],\n",
    "                desc=[ \"\" ]\n",
    "            )\n",
    "        )\n",
    "    line_source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=[0],\n",
    "                y=[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #b = p.line(x=[-74.01, -74.02], y=[40.75, 40.76])\n",
    "    \n",
    "    c = p.patches(xs =users_patches[4929]['x'], ys=users_patches[4929]['y'], alpha=0.5, fill_color=users_patches[4929]['color'])\n",
    "    s = p.square(x= users_points[4929]['x'], y=users_points[4929]['y'], size=2, color=\"black\", alpha=0.3)\n",
    "    r = p.circle('x', 'y', size=10, color='red', source=source)\n",
    "    l = p.line('x', 'y', line_width=1, color='red', source=line_source)\n",
    "    hover = HoverTool(renderers=[r], tooltips=[\n",
    "                (\"(x,y)\", \"($x, $y)\"),\n",
    "                (\"desc\", \"@desc\"),\n",
    "            ])\n",
    "\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    def update_route(lon=-74.01, lat=40.75, lon_line=[-74.01], lat_line=[40.75], travel_mode=\"None\", userid=1):\n",
    "        r.data_source.data['x'] = [lon]\n",
    "        r.data_source.data['y'] = [lat]\n",
    "        l.data_source.data['x'] = lon_line\n",
    "        l.data_source.data['y'] = lat_line\n",
    "        r.data_source.data['desc'] = [ checkCoordinate( (lon,lat),travel_mode,userid ) ]\n",
    "        p.title = checkCoordinate( (lon,lat),travel_mode,userid)\n",
    "        push_notebook()\n",
    "\n",
    "    def update_clusters(userid):\n",
    "        c.data_source.data['xs'] = users_patches[userid]['x']\n",
    "        c.data_source.data['ys'] = users_patches[userid]['y']\n",
    "        c.data_source.data['fill_color'] = users_patches[userid]['color']\n",
    "        s.data_source.data['x'] = users_points[userid]['x']\n",
    "        s.data_source.data['y'] = users_points[userid]['y']\n",
    "        push_notebook()\n",
    "\n",
    "    origin_tb = widgets.Text(description='Origin:', margin=5, value=\"soho, manhattan\")\n",
    "    desti_tb = widgets.Text(description='Destination:', margin=5, value=\"5th avenue, manhattan\")\n",
    "    btn = widgets.Button(description='Simulate', margin=5)\n",
    "    travel_tog = widgets.ToggleButtons(\n",
    "        description='Travel Modes:',\n",
    "        options=['driving', 'walking', 'bicycling','transit'],\n",
    "        margin=5\n",
    "    )\n",
    "    user_dd = widgets.Dropdown(\n",
    "        options={str(i):i for i in topuser_ids},\n",
    "        value=4929,\n",
    "        description='Clusters of User:',\n",
    "        margin=5\n",
    "    )\n",
    "    cluster_hbox1 = widgets.HBox(children=[travel_tog,user_dd])\n",
    "    cluster_hbox2 = widgets.HBox(children=[origin_tb,desti_tb,btn])\n",
    "    container = widgets.VBox(children=[cluster_hbox1,cluster_hbox2])\n",
    "\n",
    "    def btn_click(sender):\n",
    "        def get_routepath(all_steps):\n",
    "            route_path = []\n",
    "            def get_mode(temp): \n",
    "                if temp['travel_mode'] == 'TRANSIT':\n",
    "                    return 'IN ' + temp['transit_details']['line']['vehicle']['type'] + ' ' + temp['travel_mode']\n",
    "                else:\n",
    "                    return temp['travel_mode']\n",
    "            for s in all_steps:\n",
    "                if 'steps' in s:\n",
    "                    for t in s['steps']:\n",
    "                        temp = t\n",
    "                        temp['start_location']['travel_mode'] = get_mode(temp)\n",
    "                        route_path.append(temp['start_location'])\n",
    "                    temp = s['steps'][-1]\n",
    "                    temp['end_location']['travel_mode'] = get_mode(temp)\n",
    "                    route_path.append(temp['end_location'])\n",
    "                else:\n",
    "                    temp = s\n",
    "                    temp['start_location']['travel_mode'] = get_mode(temp)\n",
    "                    route_path.append(temp['start_location'])\n",
    "            temp = all_steps[-1]\n",
    "            temp['end_location']['travel_mode'] = get_mode(temp)\n",
    "            route_path.append(temp['end_location'])\n",
    "            return route_path\n",
    "\n",
    "        args = {\"origin\": origin_tb.value, \"destination\": desti_tb.value, \"mode\": travel_tog.value}\n",
    "        url = \"https://maps.googleapis.com/maps/api/directions/json?{}\".format(parse.urlencode(args))\n",
    "        route = requests.get(url).json()\n",
    "        all_steps = route['routes'][0]['legs'][0]['steps']\n",
    "        route_path = get_routepath(all_steps)\n",
    "        new_route = []\n",
    "        for idx, r in enumerate(route_path):\n",
    "            if idx == 0:\n",
    "                new_route.append(r)\n",
    "            else:\n",
    "                distance = vincenty((r['lat'],r['lng']), (route_path[idx-1]['lat'],route_path[idx-1]['lng'])).miles\n",
    "                division = 1\n",
    "                max_dist = 0.15\n",
    "                if distance > max_dist:\n",
    "                    division = math.ceil(distance/max_dist)\n",
    "                    lat_diff = (r['lat'] - route_path[idx-1]['lat'])/division\n",
    "                    lng_diff = (r['lng'] - route_path[idx-1]['lng'])/division\n",
    "                    for i in range(1,division+1):\n",
    "                        new_coord = {}\n",
    "                        new_coord['lat'] = route_path[idx-1]['lat'] + i*lat_diff\n",
    "                        new_coord['lng'] = route_path[idx-1]['lng'] + i*lng_diff\n",
    "                        new_coord['travel_mode'] = route_path[idx-1]['travel_mode']\n",
    "                        new_route.append(new_coord)\n",
    "                else:\n",
    "                    new_route.append(r)\n",
    "        route_line = {}\n",
    "        route_line['lng'] = []\n",
    "        route_line['lat'] = []\n",
    "        for r in new_route:\n",
    "            route_line['lng'].append(r['lng'])\n",
    "            route_line['lat'].append(r['lat'])\n",
    "            update_route(lon=r['lng'], lat=r['lat'], lon_line=route_line['lng'], lat_line=route_line['lat'],\n",
    "                         travel_mode=r['travel_mode'],userid=user_dd.value)\n",
    "            time.sleep(0.3)\n",
    "        return print(\"Simulation Completed\")\n",
    "\n",
    "    def dd_change(sender):\n",
    "        update_clusters(user_dd.value)\n",
    "\n",
    "    btn.on_click(btn_click)\n",
    "    user_dd.on_trait_change(dd_change)\n",
    "    display(container, show(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geolocation Clustering (social app data)\n",
    "### Simulation of user's movement,  in and out of cluster when travelling various routes\n",
    "### Recommending users different type of promotion when they are in each cluster\n",
    "<font color='red'>*rerun the cell with CTRL+Enter if the Simulate button is not working</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_geolocation_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell below performs Data Transformation (this takes a few seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datestr = '%Y-%m-%dT%H:%M:%SZ'\n",
    "def to_eastern(x):\n",
    "    return datetime.strptime(x,datestr).replace(tzinfo=timezone('utc')).astimezone(tz=timezone('US/Eastern'))\n",
    "def get_weekendday(x):\n",
    "    return 'Weekend' if x.isoweekday() in [6,7] else 'Weekday'\n",
    "\n",
    "if os.path.isfile(\"checkin_hours.pickle\"):\n",
    "    checkin_hours = pickle.load( open( \"checkin_hours.pickle\", \"rb\" ) )\n",
    "else:\n",
    "    venue = sqlContext.read.format('com.databricks.spark.csv').options(inferschema='true').load('geodata/venues.csv').rdd\n",
    "    checkin_names = checkin.map(lambda x: (int(x[8]),x)).leftOuterJoin(venue.map(lambda x: (int(x[0]),x[1]) ) ).map(lambda x: x[1][0]+[x[1][1]] )\n",
    "    checkin_hours = checkin_names.map(lambda x: ( \n",
    "         ( get_weekendday(to_eastern(x[4])),  round(to_eastern(x[4]).hour + to_eastern(x[4]).minute/60,1) ),  [x[6],x[7],x[9]] )  ).\\\n",
    "    groupByKey().mapValues(lambda x: {'y':[i[0] for i in x], 'x':[i[1] for i in x], 'name':[i[2] if i[2] else \"-\" for i in x ]}).collectAsMap()\n",
    "    pickle.dump( checkin_hours, open(\"checkin_hours.pickle\", \"wb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def geolocation_checkin_timelapse():\n",
    "    \n",
    "    hour_p = figure(title=\"Click on button \\\"Timelapse\\\" to begin\", title_text_font_size='14pt', x_range=(-74.081,-73.861), \n",
    "                    y_range=(40.7,40.865), plot_width=740, plot_height=741)\n",
    "    hour_p.image_url(\n",
    "        url=[\"https://maps.googleapis.com/maps/api/staticmap?center=Manhattan,New+York,NY&zoom=12&size=640x640&maptype=roadmap\"],\\\n",
    "                x=-74.081, y=40.865)\n",
    "\n",
    "\n",
    "    hour_source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=checkin_hours[('Weekend',0)]['x']+checkin_hours[('Weekday',0)]['x'],\n",
    "                y=checkin_hours[('Weekend',0)]['y']+checkin_hours[('Weekday',0)]['y'],\n",
    "                desc=checkin_hours[('Weekend',0)]['name']+checkin_hours[('Weekday',0)]['name']\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    hour_c = hour_p.circle('x', 'y', alpha=0.5, color='red', source=hour_source)\n",
    "\n",
    "    hour_hover = HoverTool(renderers=[hour_c], tooltips=[\n",
    "                (\"(x,y)\", \"($x, $y)\"),\n",
    "                (\"desc\", \"@desc\"),\n",
    "            ])\n",
    "    hour_p.add_tools(hour_hover)\n",
    "    \n",
    "    def decimalhour_to_hour(time_hours):\n",
    "        time_minutes = time_hours * 60\n",
    "        time_seconds = time_minutes * 60\n",
    "\n",
    "        hours_part   = floor(time_hours)\n",
    "        minutes_part = floor(time_minutes % 60)\n",
    "        seconds_part = floor(time_seconds % 60)\n",
    "        return timeclass(hours_part, minutes_part, seconds_part).strftime('%H:%M%P')\n",
    "\n",
    "    def update_hour(hour=0,day=\"Whole week\"):\n",
    "        xlist,ylist,namelist = [],[],[]\n",
    "        if day=='Weekend':\n",
    "            xlist = checkin_hours[('Weekend',hour)]['x']\n",
    "            ylist = checkin_hours[('Weekend',hour)]['y']\n",
    "            namelist = checkin_hours[('Weekend',hour)]['name']\n",
    "        elif day=='Weekday':\n",
    "            xlist = checkin_hours[('Weekday',hour)]['x']\n",
    "            ylist = checkin_hours[('Weekday',hour)]['y']\n",
    "            namelist = checkin_hours[('Weekday',hour)]['name']\n",
    "        else:\n",
    "            xlist = checkin_hours[('Weekend',hour)]['x'] + checkin_hours[('Weekday',hour)]['x']\n",
    "            ylist = checkin_hours[('Weekend',hour)]['y'] + checkin_hours[('Weekday',hour)]['y']\n",
    "            namelist = checkin_hours[('Weekend',hour)]['name'] + checkin_hours[('Weekday',hour)]['name']\n",
    "\n",
    "        hour_c.data_source.data['x'] = xlist\n",
    "        hour_c.data_source.data['y'] = ylist\n",
    "        hour_c.data_source.data['desc'] = namelist\n",
    "        hour_p.title = \"User activity during the \" + day.lower() + \" at \" + str(decimalhour_to_hour(hour))\n",
    "        push_notebook()\n",
    "\n",
    "    hour_slider = widgets.FloatSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=23.9,\n",
    "        step=0.1,\n",
    "        description='Hour of the day: ',\n",
    "        margin=5\n",
    "\n",
    "    )\n",
    "    hour_btn = widgets.Button(description='Timelapse', margin=5)\n",
    "\n",
    "    def hour_change(sender):\n",
    "        update_hour(hour_slider.value, day_select.value)\n",
    "    def hour_btn_click(sender):\n",
    "        for i in range(0,239,1):\n",
    "            update_hour(i/10)\n",
    "            hour_slider.value = i/10\n",
    "            time.sleep(0.01)\n",
    "\n",
    "    hour_slider.on_trait_change(hour_change)\n",
    "    hour_btn.on_click(hour_btn_click)\n",
    "    day_select = widgets.ToggleButtons(\n",
    "        description=\"Day of the week:\",\n",
    "        options=['Weekend','Weekday','Whole week'],\n",
    "        value='Whole week',\n",
    "        margin=5\n",
    "    )\n",
    "\n",
    "    day_select.on_trait_change(hour_change)\n",
    "\n",
    "    hbox1 = widgets.HBox(children=[hour_slider,hour_btn])\n",
    "    hbox2 = widgets.HBox(children=[day_select,hbox1])\n",
    "    display(hbox2, show(hour_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on the change in user check-ins (social app data)\n",
    "### Identify where are the key user activities during different time periods\n",
    "<font color='red'>*rerun the cell with CTRL+Enter if the TimeLapse button is not working</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geolocation_checkin_timelapse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell below performs Data Loading and Transformation (this may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SF_COORDINATES = (37.76, -122.45)  \n",
    "if os.path.isfile(\"SF_crime.pickle\"):\n",
    "    SF_crime = pickle.load( open( \"SF_crime.pickle\", \"rb\" ) )\n",
    "else:\n",
    "    SF_crime = pd.read_csv('geodata/SFPD_Incidents_2015.csv', parse_dates=['Date']).sample(frac=0.01)\n",
    "    pickle.dump( SF_crime, open(\"SF_crime.pickle\", \"wb\") )\n",
    "\n",
    "\n",
    "mc = foliumy.MarkerCluster()\n",
    "\n",
    "i = 0\n",
    "#add a marker for every record in the filtered data, use a clustered view\n",
    "for each in SF_crime.iterrows():\n",
    "\n",
    "    incident_num = each[1]['IncidntNum']\n",
    "    crime_cat = each[1]['Category']\n",
    "    crime_desc = each[1]['Descript']\n",
    "    crime_date = each[1]['Date']\n",
    "    crime_time = each[1]['Time']\n",
    "    crime_res = each[1]['Resolution']\n",
    "\n",
    "    bubble_details = IFrame(html=\"<html>Incident No: {}<br>Category: {}<br>Description: {}<br>Date: {}<br>\\\n",
    "    Time: {}<br>Resolution: {}</html>\".format(incident_num, crime_cat, crime_desc, crime_date, crime_time, crime_res),\\\n",
    "                                           width=\"300px\",height=\"150px\")\n",
    "    marker = foliumy.Marker(location=[each[1]['Y'],each[1]['X']],\n",
    "                           popup = foliumy.Popup(html=bubble_details))\n",
    "    mc.add_child(marker)\n",
    "    i+=1\n",
    "#create empty map zoomed in on San Francisco\n",
    "sf_map = foliumy.Map(location=SF_COORDINATES, zoom_start=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime distribution in San Francisco (crime data)\n",
    "### See the distribution of crime in different region of the city\n",
    "#### *Data sampled down to 1% of the actual figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf_map.add_child(mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.unique(SF_crime.Category)\n",
    "\n",
    "# create a new column in SF_crime and initialize to 0\n",
    "SF_crime['PeriodOfDay'] = 0\n",
    "\n",
    "SF_week_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "SF_week_ends = ['Saturday', 'Sunday']\n",
    "\n",
    "SF_crime.loc[SF_crime['DayOfWeek'].isin(SF_week_days), 'PeriodOfDay'] = 'WEEKDAYS'\n",
    "SF_crime.loc[SF_crime['DayOfWeek'].isin(SF_week_ends), 'PeriodOfDay'] = 'WEEKENDS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_spec_map(x,y):\n",
    "    try:\n",
    "        #definition of the boundaries in the map\n",
    "        district_sf = r'geodata/sfpddistricts.geojson'\n",
    "        \n",
    "        if os.path.isfile(\"SF_district_df.pickle\"):\n",
    "            SF_district_df = pickle.load( open( \"SF_district_df.pickle\", \"rb\" ) )\n",
    "        else:\n",
    "            if x == 'ALLDAYS':\n",
    "                SF_crime_df = SF_crime.loc[(SF_crime.Category == str(y))]\n",
    "            else:\n",
    "                SF_crime_df = SF_crime.loc[(SF_crime.PeriodOfDay == str(x)) & (SF_crime.Category == str(y))]\n",
    "\n",
    "            #unique districts and number of crimes initialized to 0\n",
    "            SF_districts = pd.unique(SF_crime.PdDistrict)\n",
    "            SF_districts = pd.DataFrame(SF_districts)\n",
    "            SF_districts.columns = ['District']\n",
    "            SF_districts['Number'] = 0\n",
    "\n",
    "            #calculating total number of incidents per district\n",
    "            SF_crime_df = pd.DataFrame(SF_crime_df['PdDistrict'].value_counts().astype(float))  \n",
    "            SF_crime_df.to_json('SF_crime_spec_agg.json')  \n",
    "            SF_crime_df = SF_crime_df.reset_index()  \n",
    "            SF_crime_df.columns = ['District', 'Number']\n",
    "\n",
    "            SF_district_df = SF_districts.merge(SF_crime_df, how=\"left\", left_on=\"District\", \\\n",
    "                                                right_on=\"District\")[['District','Number_y']].fillna(value=0)\n",
    "            SF_district_df.columns = ['District', 'Number']\n",
    "            pickle.dump( SF_district_df, open(\"SF_district_df.pickle\", \"wb\") )\n",
    "        \n",
    "        \n",
    "             \n",
    "        #creation of the choropleth\n",
    "        map_spec = foliumy.Map(location=SF_COORDINATES, zoom_start=12)  \n",
    "\n",
    "        map_spec.choropleth(geo_path = district_sf, \n",
    "                            data_out = 'SF_crime_spec_agg.json', \n",
    "                            data = SF_district_df,\n",
    "                            columns = ['District', 'Number'],\n",
    "                            key_on = 'feature.properties.DISTRICT',\n",
    "                            threshold_scale=[10,100,200,300,400,500],\n",
    "                            fill_color = 'YlOrRd', \n",
    "                            fill_opacity = 0.7, \n",
    "                            line_opacity = 0.2,\n",
    "                            legend_name = 'Number of incidents per district')\n",
    "        return map_spec\n",
    "    \n",
    "    except:\n",
    "        raise\n",
    "\n",
    "def generate_choropleth():\n",
    "    widgets.interact(generate_spec_map, x=('WEEKDAYS','WEEKENDS','ALLDAYS'), \n",
    "                     y=('NON-CRIMINAL', 'ASSAULT', 'OTHER OFFENSES', 'VANDALISM',\n",
    "           'DRUNKENNESS', 'WARRANTS', 'MISSING PERSON', 'DRUG/NARCOTIC',\n",
    "           'ROBBERY', 'LARCENY/THEFT', 'BURGLARY', 'SECONDARY CODES',\n",
    "           'RUNAWAY', 'VEHICLE THEFT', 'SEX OFFENSES, FORCIBLE',\n",
    "           'SUSPICIOUS OCC', 'WEAPON LAWS', 'SUICIDE', 'FRAUD', 'LIQUOR LAWS',\n",
    "           'TRESPASS', 'FORGERY/COUNTERFEITING', 'STOLEN PROPERTY', 'BRIBERY',\n",
    "           'EXTORTION', 'ARSON', 'DRIVING UNDER THE INFLUENCE', 'PROSTITUTION',\n",
    "           'DISORDERLY CONDUCT', 'EMBEZZLEMENT', 'KIDNAPPING',\n",
    "           'FAMILY OFFENSES', 'BAD CHECKS', 'LOITERING', 'GAMBLING',\n",
    "           'SEX OFFENSES, NON FORCIBLE', 'PORNOGRAPHY/OBSCENE MAT', 'TREA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Criminal Hotspots\n",
    "### Analysis based on time period and crime type\n",
    "#### *Data sampled down to 1% of the actual figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_choropleth()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.6.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
